# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis(RNN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QzxVa7xA6GjhpVb0zonMj3Klxgwpy3h
"""

!pip install tensorflow

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import numpy as np
import re
import nltk
import kagglehub
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint
from tensorflow.keras.models import load_model

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

jp797498e_twitter_entity_sentiment_analysis_path = kagglehub.dataset_download('jp797498e/twitter-entity-sentiment-analysis')

print('Data source import complete.')

print("Dataset path:", jp797498e_twitter_entity_sentiment_analysis_path)
print(os.listdir(jp797498e_twitter_entity_sentiment_analysis_path))

nltk.download('stopwords')

train_path = '/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv'
train_df = pd.read_csv(train_path, header=None)
val_path = '/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv'
val_df = pd.read_csv(val_path, header=None)

train_df.head()

train_df.columns = ['tweet_id', 'entity', 'sentiment', 'text']
val_df.columns = ['tweet_id', 'entity', 'sentiment', 'text']

train_df.head()

train_df['entity'].unique()

train_df['text'] = train_df['text'].fillna('').astype(str)
val_df['text'] = val_df['text'].fillna('').astype(str)

train_df.isna().sum()

def clean_tweet(tweet):
  tweet = re.sub(r'http\S+', '', tweet)
  tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)
  tweet = re.sub(r'[^\w\s]', '', tweet)
  tweet = tweet.lower()
  return tweet
stop_words = set(stopwords.words('english'))
train_df['clean_text'] = train_df['text'].apply(clean_tweet)
train_df['clean_text'] = train_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))
val_df['clean_text'] = val_df['text'].apply(clean_tweet)
val_df['clean_text'] = val_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

train_df['sentiment'].unique()

train_df['sentiment'].value_counts()

train_df = train_df[train_df['sentiment']!='Irrelevant']
val_df = val_df[val_df['sentiment']!='Irrelevant']

train_df['sentiment'] = train_df['sentiment'].str.lower()
val_df['sentiment'] = val_df['sentiment'].str.lower()

label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
train_df['label'] = train_df['sentiment'].map(label_map)
val_df['label'] = val_df['sentiment'].map(label_map)

print(train_df['label'].unique())

train_df.drop(['tweet_id', 'entity', 'sentiment'], axis=1, inplace=True)
val_df.drop(['tweet_id', 'entity', 'sentiment'], axis=1, inplace=True)

Max_words = 10000
Max_len = 50
tokenizer = Tokenizer(num_words=Max_words)
tokenizer.fit_on_texts(train_df['clean_text'])
train_sequences = tokenizer.texts_to_sequences(train_df['clean_text'])
val_sequences = tokenizer.texts_to_sequences(val_df['clean_text'])
x_train = pad_sequences(train_sequences, maxlen=Max_len)
x_val = pad_sequences(val_sequences, maxlen=Max_len)
y_train=to_categorical(train_df['label'])
y_val=to_categorical(val_df['label'])

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = ModelCheckpoint(
    filepath='best_model.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

model = Sequential([
    Embedding(input_dim=Max_words, output_dim=128, input_length=Max_len),
Dropout(0.3),
    SimpleRNN(units=128,dropout=0.3,return_sequences=False),
    Dense(64,activation='relu'),
    Dropout(0.4),
    Dense(3, activation='softmax')
    ])
model.compile(optimizer = 'adamw',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

history = model.fit(
    x_train, y_train,
    epochs=20,                  # الحد الأقصى للتدريب
    batch_size=64,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping, checkpoint],
    verbose=1
)

loss,acc = model.evaluate(x_val,y_val)
print("Loss:", loss)
print("Accuracy:", acc)

model.save('best_model1.h5')

best_model =load_model('best_model1.h5')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()
plt.plot(history.history['accuracy'],label = "Train Accuracy")
plt.plot(history.history['val_accuracy'],label = "Validation Accuracy")
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()



y_pred = best_model.predict(x_val)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_val, axis=1)

print(classification_report(y_true, y_pred_classes, target_names=['Negative', 'Neutral', 'Positive']))

def predict_sentiment(text, tokenizer, model, max_len=Max_len):
    text = clean_tweet(text)
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_len)
    pred = model.predict(padded)
    class_idx = np.argmax(pred, axis=1)[0]
    label_map_reverse = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
    return label_map_reverse[class_idx]
predict_sentiment("I love this game!", tokenizer, best_model)

import pickle
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)